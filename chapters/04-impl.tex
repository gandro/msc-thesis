\chapter{Implementation}\label{ch:impl}

This chapter consists of two parts. The first part
briefly discusses the implementation of the different components individually.
In the second part, we discuss some of the shared implementation features
of all components.

\section{Coordinator}

The richest interface in our system is implemented by the coordinator, as most
components interact with it one way or another. The coordinator basically
consists of three modules: The networking layer, the request handler and
the catalog.

\subsection{Networking Layer}

Upon initialization, the coordinator opens a TCP server socket and listens
for incoming connections. It implements a simple request/response protocol, the
implementation details of which are discussed below in section \ref{sec:reqresp}.

Every accepted connection is assigned a unique handle to the request handler.
The purpose of this handle is to track the requests a certain connection is
submitting. If this handle is dropped because the connection was closed, the
destructor of the handle makes sure to remove any shared state in the request
handler and the catalog that was bound to this the connection.

\subsection{Request Handler}

The request handler contains the central logic of the coordinator. It mutates
the state in the catalog, e.g. by adding or removing topics based on publication
and depublication requests. Not all requests can be handled immediately, a
submission request for example is only fulfilled once all processes belonging
to the query have registered themselves at the coordinator. For this reason,
the request handler also maintains some system state outside of the catalog:
These include pending submissions, blocking topic lookups or handles to the
executors in order to issue spawn requests.

Note that the request handler does not block if a request cannot be satisfied
immediately. Instead, it returns a handle for the supposed result, which will
be resolved once the necessary events for either completing or canceling the
request occur. The coordinator maintains a central event loop for this, which
is described below in section \ref{sec:futures}

\subsection{Catalog}

The purpose of the catalog is to maintain and expose the current state of the
system. \TODO{under construction}

\section{Executor}

The current implementation of the executor is relatively simple. It is a binary
that registers itself at the coordinator and then spawns child processes on behalf
of the coordinator.

In order to add a new host to the cluster, the user has to deploy the executor
binary on the new host, specify the address of the coordinator and then launch
the executor binary. The executor registers itself at the coordinator, which
also assigns a unique identifier to this executor. Once connected to the
coordinator, the executor listens for incoming spawn requests. If such an
request arrives, it downloads the binary from the specified URL and launches
it as an operating system process. Any command line arguments to be passed
to the query are provided here as well.

The executor has to inform the spawned binary about its assigned query identify,
the address of the coordinator and the address of any peer processes. In the
current implementation this is simply done using environment variables, which
can easily be accessed by the spawned query.

% TODO talk about supported url formats
% TODO talk about stdout forwarding

\section{Query Library}

We provide a small wrapper library which allows Timely processes to participate
in our system. A query submitted to the system must eventually call the 
\lstinline{timely_query::execute} function, a function which mirrors Timely's
own \lstinline{execute} function. When called, this function parses the
environment variables mentioned above which are provided by the executor.
Using this information, the process connects to the coordinator and registers
itself by providing its identifer and the group of workers it will host. Once
all worker groups have successfully registered themselves at the coordinator,
the coordinator replies with a randomized token which is used to identify
the registered processes belonging to a query. 

The initialization of the Timely worker threads and the allocation of the
communication channels among then is done using \lstinline{time\-ly_\-com\-mu\-ni\-ca\-tion}
the same way as it would be done in a standalone Timely program. For this
reason, our query library needs to translate the information provided by the
executor into Timely's own format.

Our query library also needs to provide an interface for worker threads to
publish or subscribe to topics. For this reason, every worker thread gets
a handle to the outgoing request queue. The queue handle is not directly exposed
to the user. Instead, access to the publish \& subscribe system is provided
through a set of remote procedure call stubs which are described in the
following section.

\subsection{Publishers \& Subscribers}

The query library exposes the publish and subscribe functionality. The following
API is used for publishing and subscribing to Timely streams.

\begin{lstlisting}[caption={[Publish \& subscribe interface]
The interface for publishing and subscribing Timely streams.
}]
pub struct Coordinator {
  /* hidden handle to request queue */
}

impl Coordinator {
  pub fn publish<S, D>(&self, name: &str, stream: &Stream<S, D>,
    partition: Partition) -> Result<Stream<S, D>, PublicationError>
      where D: Data + NonStatic, 
            S: Scope,
            S::Timestamp: NonStatic;

  pub fn subscribe<T, D>(&self, name: &str, cap: Capability<T>) 
    -> Result<TimelySubscription<T, D>, SubscriptionError>
      where T: Timestamp + NonStatic, 
            D: Data + NonStatic;
}
\end{lstlisting}

The \lstinline{NonStatic} bounds on the data and timestamp
parameters are required for safe serialization described in \ref{sec:serialization}.

\subsubsection{Publisher}

The \lstinline{publish} function takes a direct reference to a Timely
\lstinline{Stream<S, D>}. These stream handles, which are only available during
the dataflow graph construction phase, allow us to instantiate our own publish
operator on the stream.  The instantiated publish operator will push both, its
incoming data, as well as its current frontier to any subscribers. 

Once the operator is inserted into the dataflow graph, the \lstinline{publish}
stub issues a registration request to the coordinator, which will result in the
creation of a topic for other queries to subscribe to.

The \lstinline{partition} argument on the API specifies whether all streams
shall be merged into a single topic, of if every worker publishes its own stream.
In the second case, the identifer of the publishing worker is appended to the
name of the topic:


\begin{lstlisting}[caption={
Example use of publisher.
}]
timely_query::execute(|root, coord| {
    root.scoped::<u64, _, _>(|scope| {
        let i = scope.index();
        let numbers = (i*100..(i+1)*100).to_stream(scope);

        // results in `n` topics:
        //    "numbers.0", "numbers.1", ..., "numbers.n"
        coord.publish("numbers", &numbers, Partition::PerWorker)
             .expect("failed to publish topic");

        // filtering performed by each worker in parallel
        let primes = numbers.filter(|x| x.is_prime());

        // results in a single merged topic called "primes",
        // published by worker number 0
        coord.publish("primes", &primes, Partition::Merge)
             .expect("failed to publish topic");
    });
})
\end{lstlisting}

\subsubsection{Subscriber}

In contrast to the publisher, the current implementation of the \lstinline{subscribe}
function does not directly instantiate a Timely operator, but returns a subscription
handle which is used to read data from the topic.

There are two reasons for this choice: First, Timely requires that an operator
is instantiated on every worker. It can happen that the amount of topics a query
would like to subscribe to and the amount of workers do not match. Thus, the API
quickly becomes complicated, as we would need to provide an interface assigning
topics to operators.
Second, Timely's operator contract requires that an operator instance announces
the internal initial capabilities it holds, and, more importantly, the initial
capabilities of its peers. To support this would require some synchronization
between the subscribers, as they have to exchange the initial frontier they
observe at their topic.

Instead, we decided to base our subscription progress tracking on Timely's new
capability handles. The recently added \emph{unordered input operator} allows
a query to feed a computation from data with unordered timestamps. This operator
exposes a root capability for the earliest possible timestamp, allowing the user
to derive capabilities for newer timestamps from old ones. The frontier
is advanced by dropping the capabilities for timestamps for which no more data
is pending.

Based on the progress tracking information delivered by the publisher, the
subscription handle will automatically derive new capabilities for incoming
data and drop old ones if the frontier advances. The user however has to
provide the initial root capability for the input.

\begin{lstlisting}[caption={[Typical use of the subscription handle]
Typical use of the subscription handle. This query subscribes to a single topic of
strings, with \lstinline{u64} being the type of the timestamps.
}]
timely_query::execute(|root, coord| {

    let (mut input, cap) = root.scoped::<u64, _, _>(|scope| {
        let ((input, cap), stream) = scope.new_unordered_input();
        // stream.operators(...)
        (input, cap)
    });

    let topic = coord.subscribe::<_, String>("example", cap)
                     .expect("failed to subscribe");

    for (time, data) in topic {
        let session = input.session(time);
        session.give_content(&mut Content::Typed(data));
        root.step();
    }
})
\end{lstlisting}

In queries which use multiple workers, the query author can decide which workers
actually subscribe to topics, as the code outside the graph generation is allowed
to conditionally decide if it wants to call the subscribe function. Because
the root capability can be cloned, this interface also allows the user to
interleave the data from a topic with other data, for example by merging
multiple topics at the input.

\section{Shared implementation features}

\subsection{Networking}

Our system consists of potentially many distributed processes. These processes
are running concurrently, are dynamically added and removed from the system
and are communicating with each other. In order to deal with the inherent
complexity of such a system, we adopted an actor-model like approach for our
implementation.

Actors in our system include processes like the coordinator, the executors 
and the query processes. However, as Timely by itself could be seen as an
actor system, operators like the publish or subscribe operators are also actors
directly interacting with our system.

To support this kind of model, our networking layer also works in terms of
asynchronous messages. In contrast to Timely's own networking layer which also
provides a similar abstraction, we cannot assume a fixed number of actors.
In order to establish a connection between two components, one of them has to
take on the role of the server, while the other one acts as a client. Two
queues are allocated per connection on each actor: one for incoming messages
and one for outgoing messages. This enables messages to be sent asynchronously
in both directions. Network failures are signaled in the queue for incoming
messages. Currently, Rust's standard library TCP sockets are used as the
underlying transport mechanism, however the system could easily be extended to
support alternative transport layers as well.

\subsection{Request \& response messages} \label{sec:reqresp}

While the abstraction of single messages is sufficient for implementing the
mostly unidirectional messaging paradigm of the publish-subscribe implementation,
most other communication in the system follows a request-response pattern. Examples
for such request-response transactions are the registration of topics sent from
queries to the coordinator, or spawn requests sent from the coordinator to
executors. For this reason, we implemented a request-response multiplexer on
top of the plain message channels. This multiplexer allows both actors to have
multiple requests in flight while waiting for the corresponding responses,
which can be delivered out of order.

In order to differentiate between different kind of requests and also ensure a
well-typed response format, request payloads have to implement the \lstinline{Request}
trait. The associated name is used for decoding, while the associated \lstinline{Success}
and \lstinline{Error} types specify what kind of payloads are valid for the response.
This allows the response for a given request \lstinline{R} to be represented by
Rust's \lstinline{Result<R::Success, R::Error>} type.

\begin{lstlisting}[caption={[Request trait]In order for a type to be used as a
request message, it needs to implement the \lstinline{Request} trait. The name allows
request handlers to differentiate between different types of requests, while the associated
types forces them to issue well-formed responses.
The trait bounds are explained in section \ref{sec:serialization}.}]
pub trait Request: Abomonation + Any + Clone + NonStatic {
    type Success: Abomonation + Any + Clone + NonStatic;
    type Error: Abomonation + Any + Clone + NonStatic;

    fn name() -> &'static str;
}
\end{lstlisting}

\subsection{Serialization} \label{sec:serialization}

For both, the request/response messages as well as messages in the publish/subscribe
systems, we need to serialize the payloads in order to send them over the network.
Timely itself provides a uses a high-performance serialization library called
Abomonation, implying that all data types sent to a publish operator will be
serializable this way. This makes Abomonation a natural choice as the serialization
format for the messages sent from publishers to subscribers. 

However, Abomonation is neither memory- nor type-safe. There are no safeguards
against deserializing data into an incompatible type, which will result in undefined
behavior. In Timely, such errors are typically avoided since all worker
execute the exact same program code and the streams between workers are
statically typed. In our system however, a publisher might accidentally use
a different version of a library type than the subscriber. Because of this,
we use our own lightweight wrapper around Abomonation. In addition to the raw
serialized bytes, we annotate the buffer with the \lstinline{TypeId} of the
data type. Rust's \lstinline{TypeId} provides an opaque, globally unique
identifier for a given Rust type and its representation. Thus, we can check
if the expected and provided type identifier match before trying to deserialize
a message.

In order for a type to be serialized by our library, it needs to require the
following type bounds: \lstinline{Abomonation + Any + Clone + NonStatic}.

The \lstinline{Any} trait is required to retrieve the type's identifier. The
\lstinline{Clone} trait is used to put deserialized types into the incoming
messages queue and \lstinline{NonStatic} is an auto-trait used to disallow the
creation of eternally valid pointers into temporary buffers. We also take type
alignment rules into consideration when serializing into unaligned buffers. 

\paragraph{Future Considerations}

We also use our safe Abomonation wrapper for serializing and
deserializing requests and response messages. This currently requires all
components to be compiled for the same architecture. However, our message buffer
interface has been implemented in a way that easily allows the use of additional
serialization formats in the future. Alternative schema-based serialization
formats could also be useful in the publish/subscribe system, telling subscribers
how to deserialize the data.

\section{Dealing with asynchronous requests} \label{sec:futures}

Our components are dealing with many concurrent events from different sources
at the same time. When spawning a new query for example, the coordinator not
only has to wait for acknowledgments from the executors, it also has to be
ready to deal with the incoming connections from the newly spawned queries. And
ideally, we would like the system to be able to continue processing requests
while it waits for responses from other components.

The initial prototype of the coordinator used a continuation-passing style
for requests which could not be completed immediately. However, due to
Rust's memory ownership model, we found that this approach often resulted
in manual \emph{stack ripping} \cite{stackmgmt}, which made code both
hard to read and cumbersome to write. This lead us to consider the use of an
external library to deal with this kind of asynchronous task management.

One possible solution such library we considered is Timely Dataflow itself, which could
be used to model the coordinator as a dataflow computation.
While we think that this is certainly possible, we found the fact that Timely
has a static dataflow graph would would have required us add an additional
multiplexing layer at the inputs and outputs, complicating the code. 

In addition, Timely currently requires all data to be serializable. Because
the coordinator deals with non-serializable resources such as socket handles,
we would still require a solution to deal with asynchronous events when dealing
with I/O.

The current implementation uses the \emph{futures-rs} library for modeling
asynchronous tasks. It provides abstractions for both asynchronous computations
that yield a single result, a \lstinline{Future}, and computations that
continuously produce results which implement the \lstinline{Stream} interface
(not to be confused with Timey's stream handles). The library provides combinators
for the user to chain these asynchronous requests together or register actions
to be taken once the asynchronous request is completed.

The \emph{futures-rs} library itself however does not provide an event loop
which reacts on events and resolves pending futures. For this reason, we implemented
our own event loop.

A notable difference between \emph{futures-rs} and other future-based libraries
is that it is polling based, which fits well with Timely's execution strategy
for operators, which are also polled.
