\chapter{Discussion}\label{ch:discussion}

\section{Related Work}

\subsection{Dataflow and streaming engines}

In contrast to the standalone Timely Dataflow library, many state of the art
dataflow streaming engines have been designed and engineered from the start
to run multiple current dataflow computations in a cluster. Thus, in this
section, we will mainly focus on the execution and management aspect running
multiple dataflow computations, and not so much on the programming model these
other systems provide.

It should be noted that in contrast to Timely and our system, all of the
following systems implement some form of fault tolerance. 

\subsubsection{Spark Streaming}

Spark Streaming \cite{sparkstreaming} is a streaming engine based on the Spark
cluster computation framework. Spark Streaming executes streaming dataflow
computations by turning them into stateless, fault-tolerant batch computations.
These batches are submitted as tasks to the underlying Spark engine. \cite{spark} 

\paragraph{Process Model}

In Spark, computational tasks work are modeled as transformations on partitioned
collections of records called \emph{resilient distribute datasets}. The
runtime schedules the execution of the transformations on a distributed set of
worker nodes. By tracking the lineage of transformations, Spark is able to
recover computations by reissuing lost transformations. Based on the model,
Spark Streaming models stateful, conceptually continuous dataflow computations
as small stateless microbatches.

This model of execution is very different from Timely's continuous operator
model. In Timely, both the operator scheduling and the data of the
computation are managed directly by the workers themselves, which are hosted
by long living operating system processes. In Spark however, the data and code
is managed by the system. Because of this, Spark is not only able provide strong
fault tolerance by reissuing failed computations, Spark is also able to load
balance concurrently running streams and mitigate stranglers. While we currently
do not offer these features, we Timely does achieve much lower latencies than
Spark Streaming, which is mostly enabled by the fact that every Timely
worker manages itself independently.

%TODO driver processs

\subsubsection{Storm \& Heron}

Heron \cite{heron} is the API-compatible successor of the Storm \cite{storm}
streaming dataflow engine.
Both Storm and Heron call their dataflow computations \emph{topologies}, which 
are directed acyclic graphs of spouts (stream sources) and bolts
(stream transformers). Parallelism is achieved by the user specifying the
number of instances for each bolt and spout, as well as the partitioning
strategy between them.

\paragraph{Process Model}

The process model of Storm is very similar to our approach: A topology
(roughly corresponding to a query in our system) is executed by multiple
\emph{worker processes}. These are operating system processes which distributed
over different machines, thus similar to our query proceses. Every machine hosts
a \emph{supervisor}, which not only monitors the local worker processes, but also spawns new worker
processes on behalf of the \emph{Nimbus}, making Storm's supervisors similar to our executors.
The Nimbus is a central process to which new topologies are submitted, mirroring our coordinator.

Heron mostly differs from Storm in its internal architecture. Most notably, in
Heron every spout or bolt now runs in its own operating system process. All processes
belonging to the same topology are grouped together in operating system containers.
The motivations for this change are reported to be better debug-ability
and simpler resource management. The authors also state that their topologies
seldom have more than three stages (i.e. bolts/spouts). We believe that such
an architecture would not make much sense for Timely, as its operators are typically
more numerous and more lightweight.

Another major change from Storm's architecture
is the fact Heron does not have a central coordinating process anymore. Being
a single point of failure and a bottleneck, the Nimbus was considered to be a
flawed design. Heron thus replaces the old responsibilities of the Nimbus by
offloading them into a small set independent processes. This might be something
to consider for futures extensions to our coordinator.

\subsubsection{Flink}

Flink \cite{flink} is a dataflow streaming engine for directed acyclic
dataflow graphs, however it does have support for iterative dataflows on the
outermost level. Flink interleaves control events with data records, which
are used for progress tracking and fault tolerance, which is done through
snapshots. Progress tracking is implemented with global \emph{low watermarks}, which
denote the minimum timestamp which can be emitted at the sources of the
topology, enabling Flink to perform out-of-order processing.

\paragraph{Process Model}

The runtime architecture of Flink also uses a central process called the
\emph{Job Manager}, which accepts and manages computations submitted by
the client. The Job Manager takes the user submited code, translates it into
a dataflow graph, which can also be optimized in some cases.
The computations themselves are executed inside a worker processes called
the\emph{Task Managers}. Every task manager provides a predefined number of
task slots, which corresponds with the number of CPU cores and denotes the
amount of parallelism the task manager provides. The dataflow computation is
split up in multiple subtasks (typically corresponding with an operator), which
are placed in a certain task slot. This means that like Spark, Flink multiplexes
multiple dataflow computations within the same operating system process.
However, unlike Spark and similar to our system and Heron, these subtasks might
be long-lived, as Flink provides a continuous operator model.

\subsection{Dataflow composition}

\subsubsection{Kafka}

Kafka \cite{kafka} is a distributed platform for accumulating and sharing streams. It provides
a publish/subscribe service for potentially partitioned topics. Producers append
their data to one or more partition, allowing subscribers to read from it.
The data within a partition is stored persistently, allowing subscribers to
read all previously published data sequentially and continue where they left
of in case of failure. The data within a partition is ordered, however there
is no defined ordering across multiple partitions of the same topic. Recent
versions of Kafka also supports the assignment of event timestamps to messages.

Both Spark Streaming and Flink provide official connectors for Kafka, allowing
dataflow computations to stream data from or to Kafka topics. Flink supports
the extraction of timestamps from topics, allowing a partition
to contain out-of-order data. Similar to our system, Flink also provides a
mechanism to exact watermarks from Kafka sources, which like our system
is able to unify the progress tracking information from multiple sources.

In some ways, Kafka is very similar to our publish/subscribe as we also expose
potentially partitioned streams from which consumers, such as dataflow programs,
can subscribe to. In contrast to Kafka, we have a strict one-to-one mapping between stream
partitions and topics. Stream partitions in our system are only grouped together
by following naming conventions on the topic name. 
 
Another difference between Kafka and our system is that Kafkas subscribers are
\emph{pulling} data from their source, whereas in our system data is \emph{pushed}
from the source to the subscribers. The implication of this in our
system is that the publisher does not know anything about the progress of the
subscriber, which can lead to back-pressure issues if the subscribers are
slow.

\paragraph{Kafka Streams}
In additions to the above mentioned adapters, Kafka also provides its own
streaming engine called Kafka Streams. It provides a simple acyclic dataflow
model which uses Kafka topics and sources and sinks, thus acting as transformers.
Parallelism is achieved by instantiating the dataflow graph on multiple threads.
Partitioning of the data happens before it is fed to the individual instances
of the dataflow graph.

Like standalone Timely Dataflow, Kafka Streams is implemented as a library. It
is left to the user to deploy and launch instances of the streaming computation.

\subsubsection{MillWheel}

\section{Future Work}

\subsection{Query Resource Management and Supervision}

\subsection{Topic Buffering and Backpressure}

\subsection{Fault Tolerance}

\subsection{Automatic Query Composition}

\subsection{Dynamic Code Loading}
