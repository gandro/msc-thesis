\chapter{Timely Dataflow}\label{ch:background}

Timely Dataflow \cite{timely} (\textquote{Timely}) is a Rust library for writing distributed,
cyclic dataflow programs. It is a modular implementation of the \emph{timely dataflow
model} introduced by Naiad \cite{naiad}. 

\section{Computational Model}

Timely Dataflow is a framework for writing dataflow programs. Dataflow programming
is a programming model in which the computation can be represented as a directed graph:
The data flows along edges, while the computational logic in the vertices transforms it.
In Timely Dataflow, the vertices are also called operators, and the edges connecting
them are called streams. Notable features of the timely dataflow model in comparison
to many other dataflow systems are its support for structured loops, stateful
vertices and the option for vertices to be notified if given input rounds of
data are completed. \cite{naiad}

In the timely dataflow model, the messages flowing along edges are annotated with
timestamps. Timestamps typically denote input rounds, or within the scope of a
structured loop, the iteration count. Timestamps in timely dataflow must
define a partial order, allowing the comparison of timestamps for progress
tracking purposes: Timely offers user code to be notified when there are
no more outstanding messages for a given timestamp at a certain edge.
This allows the delivery of messages to happen out of order, while the system
is still able to inform operators about the existence of messages in-flight.
The progress tracking algorithm of Timely Dataflow is able to provide these
guarantees with minimal synchronization, allowing the computation on different
machines to advance asynchronously.

Dataflow graphs in Timely Dataflow are hierarchically structured, allowing
nested subgraphs, called scopes. A subscope can define its own clock
domain, where the timestamps of the inner scope are appended to the timestamps of messages
entering from outer scopes. When messages leave a subscope, the inner 
timestamp is stripped. Consequently, Timely implements these scoped timestamps
as product types, with the partial order derived from the combined elements.

\subsection{Data-Parallelism}

Timely employs a data-parallel approach for scaling the computation. In the
conceptual dataflow graph, the user has to specify a data partitioning scheme
on the incident edges of an operator. During execution, the dataflow graph
is instantiated on possibly many worker threads, where each worker maintains
a local copy of the dataflow graph. As data is pushed along the edges of the
conceptual graph, it is sent to the designated worker according to the
partitioning scheme. To put it another way, the data emitted by an operator
instance on one worker might be pushed to a successor operator
hosted on a different worker.

This distribution of work can result in messages being delivered out of
order. This motivates Timely to provide the progress tracking mechanism
described below, which allows operators to be argue about globally
outstanding messages.

\subsection{Progress Tracking}

A key feature of the timely dataflow model is its support for fine-grained notification
about the advancement of messages in flight, i.e. informing operators about
timestamps for which no more messages will arrive. The set of still observable
timestamps at a certain input edge at any point in time is derived from the \emph{frontier}.
A frontier $F$ restricts the set of observable future timestamps $S_F$ as follows:
$S_F = \{ t_s \mid \exists t_f \in F: t_s \geq t_f \}$ -- any future message must
have a timestamp that is greater or equal than any timestamp in the frontier.
The frontier itself is a subset of non-comparable timestamps (i.e. an antichain)
$F = \{t_1, t_2, \dots, t_n \in T \mid  t_i \parallel t_j \}$, where
$T$ is the clock domain of the input.

Operators must ask the system to be notified when the frontier advances. This
is typically done by asking for notification about the progress of messages with
a given timestamp $t$. The
notification is served when there are no more outstanding messages with
timestamp $t$ or smaller, i.e. there are no more elements in the frontier less
or equal than $t$.

In order not to break notification semantics, operators are only allowed to
produce messages with a timestamp greater than or equal to $t$ if they hold a
capapility for timestamp $t$. Operators can obtain capabilities for a certain
timestamp either during initialization of the system, or if they receive a
message of the given timestamp.

\section{Implementation}

The implementation of Timely Dataflow consists of two separate Rust libraries,
the \lstinline{time\-ly} crate which implements all functionality related to
dataflow graph construction and progress tracking, while a second library
called \lstinline{time\-ly_\-com\-mu\-ni\-ca\-tion} implements the creation and
initialization of worker threads and provides primitives for communication.
These communication primitives come in the form of asynchronous unidirectional
channels and are used by both workers and operators, though in different ways:
Workers use these channels for the exchange of progress tracking messages, while
operators use them to push data along the dataflow edges.

\subsection{Programming Interface}

The Timely Dataflow library provides a domain-specific language for expressing
dataflow graphs in Rust. Users create operators (vertices) and connect
them to other operators using stream handles (representing edges).

The Timely library contains implementations for common operators such as
\lstinline{map} or \lstinline{fil\-ter}, but also for more generic combinators
such as \lstinline{un\-ary} or \lstinline{bin\-ary} for implementing custom operators.
Many of the provided operators accept user-defined functions to implement
parts of their logic.

External input is fed to the dataflow computation using input operators. These
provide a handle for client code to push data with assigned timestamps into
the dataflow graph. Operator outputs are represented
by typed \lstinline{Stream} handles, which are used by succeeding operators
to connect the stream to their input ports.

\begin{lstlisting}[caption={[Example program in Timely Dataflow.]A trival 
single-threaded example program which reads lines from standard input,
converts them into integers and filters out odd numbers.}]
timely::execute(Configuration::Thread, |root| {
  let mut input = root.scoped(|scope| {
    let (input, stream) = scope.new_input();
      stream
        .map(|x: String| x.parse::<i32>().unwrap_or(0))
        .filter(|x| x % 2 != 0)
        .inspect(|x| println!("odd number: {:?}", x));
      input
  });

  let stdin = std::io::stdin();
  for (data, timestamp) in stdin.lock().lines().zip(1..) {
    input.send(data.unwrap());
    input.advance_to(timestamp);
    root.step();
  }
})
\end{lstlisting}

\subsection{Worker Threads}

Timely implements a data-parallel approach for its computation. This is done by
instantiating the dataflow graph on multiple worker threads, where each worker
drives the computation of its local instance. Workers can be distributed among multiple
machines, where the computation consists of multiple operating system processes
which are communicating with each other over the network. Such an
operating system process might host multiple worker threads.

\subsection{Run-time Graph Representation} \label{sec:runtime-graph}

When the execution on a worker starts, Timely assembles the run-time
dataflow graph from the the user-provided dataflow description. The run-time
representation consists of a list of operators and a list of edges between them.

When operators are instantiated, they inform the run-time about which
connections they have to other operators and provide an implementation
of the \lstinline{Operate} interface, which is used by Timely for progress
tracking and operator scheduling.

By implementing this interface, operators describe their inputs and
outputs in terms of what capabilities they initially hold and how message
timestamps can advance when passing through (the \textquote{path summary}).
The second part is needed because nested subgraphs are represented to their
parent as single operators. By requesting a path summary of every child's
internal structure, the parent has sufficient information to perform progress
tracking for its children. The operator interface is also used by parents
to inform their children about the initial capabilities and path summaries
of their surroundings.

Once construction is complete, the dataflow graph cannot be changed anymore.
Timely requires that every worker hosts exactly the same dataflow graph
structure.

During run-time, Timely's progress tracking will query the operator about its
internal progress, requiring the operator to report how many messages were
consumed and produced on each input or output edge, and also which internal
capabilities it holds on to. Notification about the frontier at the
operator's input edge is also delivered over \lstinline{Operate} interface.

A noteworthy aspect of the implementation is that the operator run-time interface
is heavily oriented towards progress tracking, it is completely decoupled
from message delivery.
As a consequence, this run-time representation of the operator graph does
currently not allow for introspection of the operators logic, the type of data
being processed or the contents of the message queues of a certain operator.
% TODO maybe mention this has an impact on operator schedulign

\subsubsection{Operator Scheduling and Execution}

Each Timely worker is responsible for scheduling the operators in its local
dataflow graph instance. Operator scheduling is cooperative and interleaved 
with progress tracking: Operators typically perform work when progress tracking
polls the operator about its internal progress. This means that progress
tracking requests generally result in the execution of any user-provided
operator logic.

Operators are always polled in a round-robin fashion, as the worker does
not know about any of the operators pending messages or other kinds of
internal work to be performed.

\subsubsection{Channel Allocation}

As Timely decouples message delivery from progress notification, there is no
need for a central message channel registry. The allocation of messaging
channels between operators is done bilaterally by the operator themselves,
using specialized endpoint allocators provided by
\lstinline{time\-ly_\-com\-mu\-ni\-ca\-tion}. Messages are typically sent in
batches, where all records within a batch share the same timestamp.

The channel allocator provided by this crate is also used by the workers for
broadcasting and receiving progress messages. Communication between workers
running in different processes communicate over TCP/IP, while workers within
the same process use thread-safe FIFO queues to communicate.
Serialization for communication over the network is done using a serialization
library called Abomonation. It trades high performance serialization and 
deserialization for type safety and requires the same in-memory data layout
at both communication endpoints.

\subsection{Deployment}

Timely allows the computation to be distributed over multiple operating system
processes. These processes host all host the same user-provided number of
worker threads an can be run on different machines. Starting a computation
involves deploying compiled binary executable on all participating machines
and spawning them it in parallel.
The amount of worker threads needs to fixed for a given execution of the
program, as all the workers are fully connected in order to be able to exchange
messages. The addresses of any peer processes is provided programmatically
or alternatively read from a text file. In all cases the worker configuration
is represented by the \lstinline{Configuration} data type shown below:

\begin{lstlisting}[caption={[Timely's worker configuration data type]
Timely's worker configuration is one the three options. The parameters in 
\lstinline{Cluster} indicates the number of threads per host, the index
of the local host, the list of all participating hosts and a boolean flag
to enable reporting.}]
pub enum Configuration {
    /// Use one thread.
    Thread,
    /// Use one process with an indicated number of threads.
    Process(usize),
    /// Expect multiple processes distributed in a cluster
    Cluster(usize, usize, Vec<String>, bool),
}
\end{lstlisting}
