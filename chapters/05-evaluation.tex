\chapter{Evaluation}\label{ch:evaluation}

For the evaluation we split a monolithic dataflow computation into a
modular set of queries. These queries publish the result of their computation
in a topic, allowing subscribers to further process the data. Because queries
can be dynamically added to the system, there is no need to interrupt the
running computation if additional analysis stages have to be added.

This increased flexibility however comes at a cost. As data has to be published
in topics in order to be accessed by consuming queries, we expect there to be
some overhead in both memory consumption and processing latency. The goal
of this chapter is to measure and analyze the overhead of query composition
with a realistic workload.

\paragraph{Experimental setup}

All experiments in this chapter are executed on a dual-socket Intel Xeon E5-2650
machine (2.0GHz, 8 cores per socket, 2 threads per core) with 64 GB RAM. All components and
queries in our system are compiled with a Rust 1.14 nightly build (2016-10-20)
and executed on Debian 7.8. The input data is read from a SAN storage attached
via iSCSI on a private 10 Gbit Ethernet network.

\section{Query composition for sessionization}

The dataflow computation in this experiment is based on \emph{sessionization},
i.e. the reconstruction of user sessions from a trace of log events.

\subsection{Workload and computation}

The workload in this experiment is a hour-long trace of log events collected in
a large datacenter owned by a travel company. The messages in our trace are
collected concurrently by 42 log servers, which receive their log events from
1263 different streams. Events are created by a distributed middleware which acts
as a message broker. It records metadata about the received messages, such as
timestamp, session identifer, and transaction identifer, and forwards them to
the log sever.

The computation we use on this workload is called sessionization. Sessionization
is the reconstruction of user sessions, i.e. grouping of all messages belonging
to the same session. For this purpose, the computation has to wait for late
messages to arrive deliveries before it can consider the session closed. In
our experiment we use a fixed inactivity limit of 5 seconds, and re-order messages
within an event time window of 10 seconds. 

While sessionization has been designed for real-time processing on a live
stream of events, we will load the workload trace from disk for our experiments.
This not only simplifies the experimental design and ensures reproducibility,
it also allows us to explore the behavior of the system if there are only a
few worker threads.

% TODO mention that even with the overhead we are able to keep up in real time

\subsection{Dataflow graph}

Input is read from disk and fed into the dataflow graph in parallel, each worker
is responsible for a partition of the log servers. The read messages are
re-ordered in a 10 second according to their event time to ensure all messages 


We partition the message trace over the workers and read in parallel
through a re-order buffer.
After the sessionization, the computation performs a set of statistics on the
reconstructed sessions. The stages are described below, the resulting dataflow
graph is shown in Figure~\ref{fig:monolith}

\begin{description}
\item [Input] Parses the input files and feeds them into the sessionization stage.
\item [Sessionization] Distributes and then groups messages according to their session identifier.
Emits groups of messages belonging to the same session after a period of inactivity.
\item [Messages per Session] Simply counts and emits the number of messages belonging to a session.
\item [Duration of Session] Calculates the timespan between the first and last message of a session.
\item [Parse Transaction Trees] Parses the transaction identifiers for tree extraction.
\item [Transaction Tree Depth] Calculates the depth of the transaction trees.
\item [Top k Transaction Tree Signatures] Calculates signatures for the transaction trees and
reports the ten top most common tree signatures.
\item [Top k Communication Pairs] Infers the ten most common pairs of services who are
communicating with each other.
\item [Output] Collects emitted data in a histogram.
\end{description}

Messages are first shuffeled according to their session identifier for the
sessionization stage, and are processed in parallel until statistics are
collected at a single worker for the output.

\subsubsection{Splitting up the dataflow graph}

For this evaluation we split up the computation such that each stage represents
its own sub-query. In order to achieve this, we introduce two topics: The topic
\emph{sessionize} contains the result of the sessionization stage, i.e. 
the groups of messages belonging to a session. The second topic, named
\emph{transactions}, is introduced for the transaction-oriented statistics.
It emits a stream containing all the transactions per session. The resulting
composed dataflow graph is shown in Figure~\ref{fig:split}, it consists of
seven queries, each one running in its own operating system process.

\begin{figure}[p]
  \centering
    \includegraphics[width=1\textwidth]{figures/sessionize_dataflow-crop}
  \caption[Dataflow graph for monolithic sessionization]{The dataflow graph
  for the original, monolithic sessionization query.}
  \label{fig:monolith}
\end{figure}

\begin{figure}[p]
  \centering
    \includegraphics[width=1\textwidth]{figures/sessionize_split-crop}
  \caption[Dataflow graph for modular sessionization]{Structure of the modular
  sessionization. Two topics are introduced connect the tree of queries.}
  \label{fig:split}
\end{figure}

\paragraph{Topic Partitioning}

We maintain a simple one-to-one mapping between the number of topics and
number of worker threads in the subscribers for the composed query.
Merging the stream partitions for publication and redistributing
the data at each subscriber would just result in unnecessary work.

% TODO show distribution inside topics

\subsection{Worker-to-processor mapping}

Because we use the same amount of workers for each query, but the composed
version naturally consists of more than one query, the execution of the
composed version will spawn more operating system threads than
the monolithic version.

For this reason we limit the the available CPU cores for the different executions,
ensuring the number of workers per query matches the number of available CPU cores.
Because our machine does have two processor sockets, each with their own memory bank,
the question arises how performance is impacted by different processor placement
policies. We explore this in section~\ref{sec:evalnuma}.

Based on this, we prefer using as many physical cores as possible before falling back
to hyper-threads, and we prefer filling the first socket before allocating
threads on the second socket. The memory allocation policy is set up to
always allocate on the local node.

Note that we do not pin individual worker threads to individual cores, we just
limit the access to additional cores. The operating system is free to load-balance
threads between the available cores.

\section{Results}


\subsection{Overhead for different number of workers}

We report the overall execution time of of both the monolithic and the 
modular query for different amounts of worker threads in Figure~\ref{fig:times}.

\begin{figure}[htb]
  \centering
    \includegraphics[width=1\textwidth]{figures/evaluation/times}

    {\footnotesize
    \vspace{1em}
    \begin{tabularx}{\textwidth}{ rXXXXXX }
      \hline 
      \textsc{Threads} & 4 & 8 & 12 & 16 & 24 & 32 \\
      \hline 
      \textsc{Overhead} & 17.2\%&13.7\%&14.0\%&18.8\%&14.4\%&13.5\% \\
      \hline
    \end{tabularx}
    }
    \caption[Execution times for different number of workers]{
    Execution times for different number of workers, for both the monolithic
    and the composed version.}
    \label{fig:times}
\end{figure}

As expected, we see that the modular version is slower than its monolithic
counterpart. Where the monolithic query uses in-memory queues to send data
to the next stage, the participants of composed version have to serialize
the messages and publish them in a topic. Both versions don't necessarily
perform better if we use hyper-threading for additional worker threads. 

Note that the relative overhead is not proportional to the number of
workers. We show in section~\ref{sec:evalnuma} that worker placement also
impacts the overhead.

\subsection{Worker utilization factor}

In this section we present the average measured utilization per worker, i.e. the proportion
of time a worker performs useful work. This gives us an indicator how much load
is put on the different queries.

We define the utilization factor of a worker as the percentage of
the time spent inside the Timely computation, i.e.
$\text{utilization} = \frac{\text{busy time}}{\text{total time}}$.
If a worker is not busy, it waiting on external input. This means either from a
topic in the case of the subscribers, or from the external input from disk in
the case of the sessionization stage.

Figure~\ref{fig:subutil} shows the average utilization for the subscribers, while
Figure~\ref{fig:sessutil} shows the utilization for the whole monolithic query
compared to the sessionization stage of the modular query. 

We see that in all cases the average utilization for the sessionization stage
is much higher than for the other queries in the modular version, indicating
that it sessionization itself the bottleneck of the computation. This trend
increases with the number of threads. With many threads, the he workers of
the subscribing queries are idle waiting for input most of the time.

This suggests that in a realistic scenario we would not want to use the same
amount of worker threads for all queries. Future experiments would have to
explore how many worker threads are actually necessary for the different queries
in order to keep up. This however also requires a more sophisticated assignment of
topics to subscribing workers, which currently has to be manually encoded in
the queries source code.

\begin{figure}[p]
  \centering
    \includegraphics[width=1\textwidth]{figures/evaluation/subsutil}
    \caption[Subscriber worker utilization]{Utilization for the individual
    subscribers in the modular version.}
    \label{fig:subutil}
\end{figure}

\begin{figure}[p]
  \centering
    \includegraphics[width=1\textwidth]{figures/evaluation/sessutil}
    \caption[Sessionization worker utilization]{Utilization of the sessionization stage
    in the monolithic and the modular version.}
    \label{fig:sessutil}
\end{figure}
\clearpage
\subsection{Memory consumption and processor utilization}

Figures \ref{fig:rss} and \ref{fig:cpu} report the processor utilization and peak
memory consumption for the different processes, as reported by the operating
system.

\paragraph{Memory consumption}

We immediately see that the sessionization query of the composed version
consumes the most memory. This is caused by the fact that the re-ordering
and sessionization buffers incoming messages until certain epochs are completed.
Also with regard to the sessionization query, we see that its memory footprint
is higher than memory consumption for the monolithic version, even though
it performs less work. We assume that this is caused by the queuing of
outgoing messages at the publisher.

Another contributing factor to the overhead is the fact the we run additional
operating system processes, each one with a large set of worker threads.

\TODO{maybe split the figure in two separate figures, one for the subscribers
and one for sessionization}

\paragraph{Processor utilization}

We also report the overall processor utilization reported by the operating system.
A value of 100\% would correspond to the full utilization of 32 cores. We see that
neither versions manage to fully utilize the processors over then duration of
the whole computation. This is caused by the fact that both versions have to wait
for external input. Corresponding to the observations from worker utilization, we
also see that the sessionization stage is the most compute heavy.

\begin{figure}[p]
  \centering
    \includegraphics[width=1\textwidth]{figures/evaluation/rss}

    {\footnotesize
    \vspace{1em}
    \begin{tabularx}{\textwidth}{ rXXXXXX }
      \hline 
      \textsc{Threads} & 4 & 8 & 12 & 16 & 24 & 32 \\
      \hline 
      \textsc{Overhead} & 7.9\%&10.3\%&10.6\%&9.7\%&13.7\%&13.5\%\\
      \hline
    \end{tabularx}
    }
    \caption[Peak memory consumption]{Peak memory consumption of the different
    processes. The left bar shows the monolithic query, the right bar shows the memory
    used per query processed of the composed version.}
    \label{fig:rss}
\end{figure}

\begin{figure}[p]
  \centering
    \includegraphics[width=1\textwidth]{figures/evaluation/cpu_tot}
    \caption[Total CPU utilization]{CPU utilization for the different
    processes. The left bar shows the monolithic query, the right bar shows the
    processor utilization for the different query processes in the modular version.}
    \label{fig:cpu}
\end{figure}
\clearpage
\subsection{Impact of worker placement} \label{sec:evalnuma}

In the above experiments, we prefer running as many workers as possible
on a single processor socket (with the exception of hyper-threads).
Figure~\ref{fig:numa} compares execution time of this strategy compared to a
strategy where we balance the workers evenly between sockets. With 12 workers
we are unable put all threads on a single socket, and we see that in this case
it is slightly beneficial to balance the threads evenly.

The impact of using different placements is relatively small for the
monolithic query. The modular version however is more sensitive to
different placement strategies, resulting in changes to the overhead
of the execution time. 

\begin{figure}[h]
  \centering
    \includegraphics[width=1\textwidth]{figures/evaluation/numa}

    {\footnotesize
    \vspace{1em}
    \begin{tabularx}{\textwidth}{ r|XX|XX|XX }
      \hline 
      \textsc{Threads} & \multicolumn{2}{c|}{4} & \multicolumn{2}{c|}{8} & \multicolumn{2}{c}{12} \\
      \hline 
      \textsc{Placement} & (2, 2)&(4, 0)&(4, 4)&(8, 0)&(6, 6)&(8, 4)\\
      \hline 
      \textsc{Overhead} & 22.9\%&17.2\%&17.9\%&13.7\%&17.3\%&14.0\% \\
      \hline
    \end{tabularx}
    }
    \caption[Execution time for different of worker placement]{Execution times for
    different worker-to-processor mappings. The placement describes the number of
    worker threads placed on each of the two processor sockets.}
    \label{fig:numa}
\end{figure}

